{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Toxic_Comment_Classifier.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVSp2TM7FBRD"
      },
      "source": [
        "# Toxicity Classification:\n",
        "\n",
        "### 1. Problem:\n",
        "\n",
        "**Source:** https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification<br><br>\n",
        "**Description:** https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/description<br><br>\n",
        "**Problem Statement:** Given a comment made by the user, predict the toxicity of the comment.<br><br>\n",
        "\n",
        "\n",
        "#### 2 Data: Source <br>\n",
        "\n",
        "- Source: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAJp34I0FBRK"
      },
      "source": [
        "## Importing Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVyEHFVPFBRL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, PorterStemmer\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import gensim\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "import warnings\n",
        "from keras import backend as K\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI85Ey6-SMQj",
        "outputId": "58885d0c-2ba1-414b-a75a-b2188c9015d9"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNRiKaaJFBRN"
      },
      "source": [
        "## Reading Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ6JNYaXtRW1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "5dAghlMAFBRN",
        "outputId": "d64ff0d2-991c-4a90-9791-3646029eb3ab"
      },
      "source": [
        "train_df = pd.read_csv('train.csv', index_col='id', engine='python')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m   3015\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: unexpected end of data",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-dc25a089ac85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   2556\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2558\u001b[0;31m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2559\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_get_lines\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m   3303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3304\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3305\u001b[0;31m                             \u001b[0mnew_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_iter_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3306\u001b[0m                             \u001b[0mrows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m   3037\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\". \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3039\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alert_malformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3040\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_alert_malformed\u001b[0;34m(self, msg, row_num)\u001b[0m\n\u001b[1;32m   2996\u001b[0m         \"\"\"\n\u001b[1;32m   2997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2998\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2999\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_bad_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m             \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Skipping line {row_num}: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: unexpected end of data"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3rWc12JFBRN"
      },
      "source": [
        "test_df = pd.read_csv('test.csv', index_col='id', engine='python')\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JW3LOkvFBRO"
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDCzc-O-FBRO"
      },
      "source": [
        "**Looking at count values of asian,atheist etc columns we see that there are a lot of Null values present, but this is OK as we are only supposed to use 'comment_text' column so dealing with these is not really necessary here.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIt40veLFBRO"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HoEt3TiFBRP"
      },
      "source": [
        "train_df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwuVx8tDFBRP"
      },
      "source": [
        "**No Null values for target and comment_text columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyt2u9VkFBRP"
      },
      "source": [
        "print(\"Train and test shape: {} {}\".format(train_df.shape, test_df.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTqgiVyKFBRQ"
      },
      "source": [
        "## Exploratory Data Analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jeqPsaWFBRQ"
      },
      "source": [
        "### 1. Target Feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOX6v5HsFBRQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "b4ad7813-3154-425f-b682-5899efca633f"
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.title(\"Target value Distributions\")\n",
        "sns.distplot(train_df['target'], kde=True, hist=False, bins=240, label='target')\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e072d358cf93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target value Distributions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ6F4d57FBRQ"
      },
      "source": [
        "**We see that most of the comments present in the dataset are actually non-toxic (<0.5) and only a few of them are actually toxic (>0.5)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20S14idjFBRR"
      },
      "source": [
        "# If toxicity rating < 0.5 then the comment is non-toxic else it is toxic.\n",
        "# Get toxic and non-toxic comments.\n",
        "temp = train_df['target'].apply(lambda x: \"non-toxic\" if x < 0.5 else \"toxic\")\n",
        "\n",
        "# Plot the number and percentage of toxic and non-toxic comments.\n",
        "fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
        "total = float(len(temp))\n",
        "\n",
        "# Plot the count plot.\n",
        "cntplot = sns.countplot(temp)\n",
        "cntplot.set_title('Percentage of non-toxic and toxic comments')\n",
        "\n",
        "# Get the height and calculate percentage then display it the plot itself.\n",
        "for p in ax.patches:\n",
        "    # Get height.\n",
        "    height = p.get_height()\n",
        "    # Plot at appropriate position.\n",
        "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXxRkN8dFBRR"
      },
      "source": [
        "**The dataset is imbalanced as 92% of the comments are non-toxic and only 8% are toxic**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHxTgZ4wFBRR"
      },
      "source": [
        "### 2. Toxicity Subtype Features:\n",
        "- severe_toxicity\n",
        "- obscene\n",
        "- threat\n",
        "- insult\n",
        "- identity_attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8V5fvt0FBRS"
      },
      "source": [
        "# Code from: https://www.kaggle.com/gpreda/jigsaw-eda\n",
        "# Modified a bit according to my needs\n",
        "def plot_features_distribution(features, title, data):\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.title(title)\n",
        "    for feature in features:\n",
        "        sns.distplot(data[feature],kde=True,hist=False, bins=240, label=feature)\n",
        "    plt.xlabel('')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q7RMP1IFBRS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "36c438ab-1b75-49f0-df95-1f06a5df21ba"
      },
      "source": [
        "features = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
        "plot_features_distribution(features, \"Distribution of additional toxicity features in the train set\", train_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-63fde4cf8c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'severe_toxicity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'obscene'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'identity_attack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'insult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'threat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_features_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Distribution of additional toxicity features in the train set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRStxEc-FBRS"
      },
      "source": [
        "# Looking at the distribution of additional toxicity features on the comments that are actually considered toxic:\n",
        "temp = train_df[train_df['target'] > 0.5]\n",
        "plot_features_distribution(features, \"Distribution of additional toxicity features in only toxic comments data\", temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M17H7yF4FBRS"
      },
      "source": [
        "**We see that for toxic comments data, there are more insulting comments as compared to obscene comments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWR5rJZ4FBRT"
      },
      "source": [
        "# Getting the count of additonal toxicity features in toxic comments data(temp):\n",
        "def get_comment_nature(row):\n",
        "    # Extract type of toxic comment\n",
        "    row = [row['severe_toxicity'], row['obscene'], row['identity_attack'], row['insult'], row['threat']]\n",
        "    \n",
        "    maxarg = np.argmax(np.array(row)) # Get the max value index.\n",
        "    \n",
        "    if maxarg == 0: return 'severe_toxicity'\n",
        "    elif maxarg == 1: return 'obscene'\n",
        "    elif maxarg == 2: return 'identity_attack'\n",
        "    elif maxarg == 3: return 'insult'\n",
        "    else: return 'threat'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOPfAufQFBRT"
      },
      "source": [
        "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
        "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
        "total = float(len(x))\n",
        "\n",
        "# Plot the count plot.\n",
        "cntplot = sns.countplot(x)\n",
        "cntplot.set_title('Percentage of toxicity nature in toxic comments data')\n",
        "\n",
        "# Get the height and calculate percentage then display it the plot itself.\n",
        "for p in ax.patches:\n",
        "    # Get height.\n",
        "    height = p.get_height()\n",
        "    # Plot at appropriate position.\n",
        "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZWd_We3FBRU"
      },
      "source": [
        "**In our train dataset only 8% of the data was toxic. Out of that 8%, 81% of the toxic comments made are insults, 8.37% are identity attacks, 7.20% are obscene, 3.35% are threats and a very small amount of toxic comments are severly toxic.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JxlwM2tFBRU"
      },
      "source": [
        "### 3. Identity Attributes:\n",
        "\n",
        "Sensitive topics:\n",
        "\n",
        "- male\n",
        "- female\n",
        "- homosexual_gay_or_lesbian\n",
        "- bisexual\n",
        "- heterosexual\n",
        "- christian\n",
        "- jewish\n",
        "- muslim\n",
        "- black\n",
        "- white\n",
        "- asian\n",
        "- latino"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlK_mIiJFBRU"
      },
      "source": [
        "temp = train_df.dropna(axis = 0, how = 'any')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeWwBcyIFBRV"
      },
      "source": [
        "features = ['male', 'female', 'transgender', 'other_gender']\n",
        "plot_features_distribution(features, \"Distribution of gender feature values\", temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu1RsIjeFBRV"
      },
      "source": [
        "features = ['bisexual', 'heterosexual', 'homosexual_gay_or_lesbian', 'other_sexual_orientation']\n",
        "plot_features_distribution(features, \"Distribution of sexual orientation features values in the train set\", temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0qN5AtGFBRV"
      },
      "source": [
        "features = ['asian', 'black', 'jewish', 'latino', 'other_race_or_ethnicity', 'white']\n",
        "plot_features_distribution(features, \"Distribution of race and ethnicity features values in the train set\", temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCfO0QnBFBRV"
      },
      "source": [
        "# Get data where race/ethnic references are made.\n",
        "cond = (train_df['asian'] > 0.5) | (train_df['black'] > 0.5) | (train_df['jewish'] > 0.5) | (train_df['latino'] > 0.5) | (train_df['white'] > 0.5)\n",
        "temp = train_df[cond] # Get data where race/ethnic references are made.\n",
        "temp = temp[temp['target'] > 0.5] # Extract only toxic comments.\n",
        "\n",
        "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
        "total = float(len(x))\n",
        "\n",
        "# Plot the count plot.\n",
        "cntplot = sns.countplot(x)\n",
        "cntplot.set_title('Percentage of type of toxicity in comments where race/ethnic references are made')\n",
        "\n",
        "# Get the height and calculate percentage then display it the plot itself.\n",
        "for p in ax.patches:\n",
        "    # Get height.\n",
        "    height = p.get_height()\n",
        "    # Plot at appropriate position.\n",
        "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TowhUadtFBRW"
      },
      "source": [
        "**We see that the toxic comments involving words like black, asian etc. are mainly used for identity attacks or insults.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5HojDlXFBRW"
      },
      "source": [
        "# Get data where race/ethnic references are made.\n",
        "cond = (train_df['bisexual'] > 0.5) | (train_df['heterosexual'] > 0.5) | (train_df['homosexual_gay_or_lesbian'] > 0.5) | (train_df['other_sexual_orientation'] > 0.5) \n",
        "temp = train_df[cond] # Get data where race/ethnic references are made.\n",
        "temp = temp[temp['target'] > 0.5] # Extract only toxic comments.\n",
        "\n",
        "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
        "total = float(len(x))\n",
        "\n",
        "# Plot the count plot.\n",
        "cntplot = sns.countplot(x)\n",
        "cntplot.set_title('Percentage of type of toxicity in comments where sexual orientation references are made')\n",
        "\n",
        "# Get the height and calculate percentage then display it the plot itself.\n",
        "for p in ax.patches:\n",
        "    # Get height.\n",
        "    height = p.get_height()\n",
        "    # Plot at appropriate position.\n",
        "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OXpc7TlFBRX"
      },
      "source": [
        "**We see from the plot that the toxic comments where sexual orientation references are made are mostly used for identity attacks.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO3TzgafFBRX"
      },
      "source": [
        "# Get data where race/ethnic references are made.\n",
        "cond = (train_df['male'] > 0.5) | (train_df['female'] > 0.5) | (train_df['transgender'] > 0.5) | (train_df['other_gender'] > 0.5) \n",
        "temp = train_df[cond] # Get data where race/ethnic references are made.\n",
        "temp = temp[temp['target'] > 0.5] # Extract only toxic comments.\n",
        "\n",
        "x = temp.apply(get_comment_nature, axis=1) # Get nature of each toxic comment\n",
        "\n",
        "fig, ax = plt.subplots(1,1,figsize=(7,7))\n",
        "total = float(len(x))\n",
        "\n",
        "# Plot the count plot.\n",
        "cntplot = sns.countplot(x)\n",
        "cntplot.set_title('Percentage of type of toxicity in comments where gender references are made')\n",
        "\n",
        "# Get the height and calculate percentage then display it the plot itself.\n",
        "for p in ax.patches:\n",
        "    # Get height.\n",
        "    height = p.get_height()\n",
        "    # Plot at appropriate position.\n",
        "    ax.text(p.get_x() + p.get_width()/2.0, height + 3, '{:1.2f}%'.format(100*height/total), ha='center')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_L0LZLaFBRX"
      },
      "source": [
        "**From the plot we see that the toxic comments which involve words like male, female etc are insults.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXnRGgmXFBRX"
      },
      "source": [
        "### 4. Features generated by users feedback:\n",
        "\n",
        "- funny\n",
        "- sad\n",
        "- wow\n",
        "- likes\n",
        "- disagree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVvnxjbXFBRY"
      },
      "source": [
        "# Code from: https://www.kaggle.com/gpreda/jigsaw-eda\n",
        "# Modified a bit\n",
        "def plot_count(feature, title, data, size=1):\n",
        "    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n",
        "    total = float(len(data))\n",
        "    g = sns.countplot(data[feature], order = data[feature].value_counts().index[:20], palette='Set3')\n",
        "    g.set_title(\"Number and percentage of {}\".format(title))\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.text(p.get_x()+p.get_width()/2.,\n",
        "                height + 3,\n",
        "                '{:1.2f}%'.format(100*height/total),\n",
        "                ha=\"center\") \n",
        "    plt.show()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxYP41SDFBRY"
      },
      "source": [
        "plot_count('funny','funny votes given', train_df, 3)\n",
        "plot_count('funny', 'funny votes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqArg17zFBRY"
      },
      "source": [
        "plot_count('sad','sad votes given', train_df, 3)\n",
        "plot_count('sad', 'sad votes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRXU3gBkFBRY"
      },
      "source": [
        "plot_count('wow','wow votes given', train_df, 3)\n",
        "plot_count('wow', 'wow votes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuUdR07uFBRZ"
      },
      "source": [
        "plot_count('likes','likes given', train_df, 3)\n",
        "plot_count('likes', 'likes given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9i3sJnZFBRZ"
      },
      "source": [
        "plot_count('disagree','disagree given', train_df, 3)\n",
        "plot_count('disagree', 'disagree given on toxic comments only', train_df[train_df['target'] > 0.5], 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rfZKfriFBRZ"
      },
      "source": [
        "### 5. Comments_text Feature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsd3N-LwFBRZ"
      },
      "source": [
        "# Code From: https://www.kaggle.com/gpreda/jigsaw-eda\n",
        "stpwrds = set(STOPWORDS)\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stpwrds,\n",
        "        max_words=50,\n",
        "        max_font_size=40, \n",
        "        scale=5,\n",
        "        random_state=1\n",
        "    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize=(10,10))\n",
        "    plt.axis('off')\n",
        "    if title: \n",
        "        fig.suptitle(title, fontsize=20)\n",
        "        fig.subplots_adjust(top=2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm3WUiL_FBRa"
      },
      "source": [
        "show_wordcloud(train_df['comment_text'].sample(20000), title = 'Prevalent words in comments - train data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuNFDNTJFBRa"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['insult'] > 0.75]['comment_text'].sample(20000), \n",
        "               title = 'Prevalent comments with insult score > 0.75')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdYfmUtXFBRa"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['threat'] > 0.75]['comment_text'], \n",
        "               title = 'Prevalent words in comments with threat score > 0.75')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L97wAaGRFBRa"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['obscene'] > 0.75]['comment_text'], \n",
        "               title = 'Prevalent words in comments with obscene score > 0.75')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWW5ihduFBRb"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['target'] > 0.75]['comment_text'], \n",
        "               title = 'Prevalent words in comments with target score > 0.75')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekzZdTt8FBRb"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['target'] < 0.25]['comment_text'], \n",
        "               title = 'Prevalent words in comments with target score < 0.25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNu_TFRSFBRb"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['obscene']< 0.25]['comment_text'], \n",
        "               title = 'Prevalent words in comments with obscene score < 0.25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo9a1qIdFBRb"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['threat'] < 0.25]['comment_text'], \n",
        "               title = 'Prevalent words in comments with threat score < 0.25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQmNfPBBFBRc"
      },
      "source": [
        "show_wordcloud(train_df.loc[train_df['insult'] < 0.25]['comment_text'].sample(20000), \n",
        "               title = 'Prevalent comments with insult score < 0.25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VTuG4h6FBRc"
      },
      "source": [
        "## Preprocessing Text and Train-Test Split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJCbk7JgFBRc"
      },
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess(text_string):\n",
        "    text_string = text_string.lower() # Convert everything to lower case.\n",
        "    text_string = re.sub('[^A-Za-z0-9]+', ' ', text_string) # Remove special characters and punctuations\n",
        "    \n",
        "    x = text_string.split()\n",
        "    new_text = []\n",
        "    \n",
        "    for word in x:\n",
        "        if word not in stop_words:\n",
        "            new_text.append(stemmer.stem(word))\n",
        "            \n",
        "    text_string = ' '.join(new_text)\n",
        "    return text_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLHytAehFBRc"
      },
      "source": [
        "%%time\n",
        "train_df['preprocessed_text'] = train_df['comment_text'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohtdzQ34FBRd"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut6Fb8v4FBRd"
      },
      "source": [
        "%%time\n",
        "test_df['preprocessed_text'] = test_df['comment_text'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvNEmwf3FBRd"
      },
      "source": [
        "feature = train_df[['preprocessed_text']]\n",
        "output = train_df[['target']]\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(feature, output)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_cv.shape)\n",
        "print(y_train.shape)\n",
        "print(y_cv.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp658TPbFBRd"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofDyZU3oFBRe"
      },
      "source": [
        "X_cv.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGbKqvwCFBRe"
      },
      "source": [
        "X_test = test_df[['preprocessed_text']]\n",
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTH9KhR6FBRe"
      },
      "source": [
        "# Saving the files to csv so that we dont need to preprocess again.\n",
        "X_train.to_pickle('X_train.pkl')\n",
        "X_cv.to_pickle('X_cv.pkl')\n",
        "X_test.to_pickle('X_test.pkl')\n",
        "y_train.to_pickle('y_train.pkl')\n",
        "y_cv.to_pickle('y_cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeLqSUpJFBRe"
      },
      "source": [
        "## Training Models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIGS6ai7FBRe"
      },
      "source": [
        "# To load the csv files:\n",
        "# Uncomment the block.\n",
        "X_train = pd.read_pickle('X_train.pkl')\n",
        "X_cv = pd.read_pickle('X_cv.pkl')\n",
        "X_test = pd.read_pickle('X_test.pkl')\n",
        "y_train = pd.read_pickle('y_train.pkl')\n",
        "y_cv = pd.read_pickle('y_cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vka8TQyeFBRf"
      },
      "source": [
        "### 1. Bag of Words (BoW):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQClNT2PFBRf"
      },
      "source": [
        "cnt_vec = CountVectorizer(ngram_range=(1,2), max_features=30000)\n",
        "bow_train = cnt_vec.fit_transform(X_train['preprocessed_text'])\n",
        "bow_cv = cnt_vec.transform(X_cv['preprocessed_text'])\n",
        "bow_test = cnt_vec.transform(X_test['preprocessed_text'])\n",
        "\n",
        "print(bow_train.shape)\n",
        "print(bow_cv.shape)\n",
        "print(bow_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr4duvnNFBRf"
      },
      "source": [
        "#### 1.1 SGDRegressor:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AajaxF3FBRf"
      },
      "source": [
        "##### 1.1.1 Hyperparameter Tuning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci_yJWe3FBRf"
      },
      "source": [
        "# Performing hyperparameter tuning:\n",
        "alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "penalty = ['l1', 'l2']\n",
        "xticks = []\n",
        "tr_errors = []\n",
        "cv_errors = []\n",
        "best_model = None\n",
        "best_error = 100\n",
        "for a in alpha:\n",
        "    for p in penalty:\n",
        "        xticks.append(str(a) + ' ' + p)\n",
        "        print(str(a) + ' ' + p + \" :\")\n",
        "        \n",
        "        model = SGDRegressor(alpha=a, penalty=p) \n",
        "        model.fit(bow_train, y_train) # Train\n",
        "        \n",
        "        preds = model.predict(bow_train) # Get predictions\n",
        "        err = mean_squared_error(y_train['target'], preds) # Calculate error on trainset\n",
        "        tr_errors.append(err)\n",
        "        print(\"Mean Squared Error on train set: \", err)\n",
        "        \n",
        "        preds = model.predict(bow_cv) # Get predictions on CV set\n",
        "        err = mean_squared_error(y_cv['target'], preds) # Calculate error on cv set\n",
        "        cv_errors.append(err)\n",
        "        print(\"Mean Squared Error on cv set: \", err)\n",
        "        \n",
        "        if err < best_error: # Get best model trained\n",
        "            best_error = err\n",
        "            best_model = model\n",
        "        \n",
        "        print(\"*\"*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETKY6hKxFBRg"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.suptitle(\"Hyperparameters vs MSE\")\n",
        "plt.plot(range(len(alpha) * len(penalty)), tr_errors)\n",
        "plt.plot(range(len(alpha) * len(penalty)), cv_errors)\n",
        "plt.legend(['train', 'cv'])\n",
        "plt.xticks(range(len(alpha) * len(penalty)), xticks, rotation=45)\n",
        "plt.xlabel('Hyperparameter: alpha + penalty')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eyg1gm3FBRg"
      },
      "source": [
        "# Getting the best model parameters:\n",
        "best_model.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrNMrBOKFBRg"
      },
      "source": [
        "##### 1.1.2 Feature Importance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTTlSgXwFBRg"
      },
      "source": [
        "# Printing the 20 most important features/words which contribute to a comment being toxic.\n",
        "feat_names = cnt_vec.get_feature_names()\n",
        "weights = best_model.coef_\n",
        "df = pd.DataFrame(data=weights, columns=['weights'], index=feat_names)\n",
        "df.sort_values(\"weights\", ascending=False).iloc[0:20,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Hoc7hHFBRh"
      },
      "source": [
        "# 20 most important features/words which contribute to comment being non-toxic.\n",
        "df.sort_values(\"weights\", ascending=True).iloc[0:20,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMqcGkXMFBRh"
      },
      "source": [
        "#### 1.2 Decision Trees:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTnHEv4gFBRh"
      },
      "source": [
        "##### 1.2.1 Hyperparameter Tuning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWjzu_kuFBRh"
      },
      "source": [
        "# Performing hyperparameter tuning:\n",
        "max_depth = [3, 5, 7]\n",
        "min_samples = [10, 100, 1000]\n",
        "xticks = []\n",
        "tr_errors = []\n",
        "cv_errors = []\n",
        "best_model = None\n",
        "best_error = 100\n",
        "for d in max_depth:\n",
        "    for samp in min_samples:\n",
        "        xticks.append(\"Depth- \" + str(d) + ' Min Samples leaf-' + str(samp))\n",
        "        print(\"Depth- \" + str(d) + ' Min Samples leaf-' + str(samp) + \" :\")\n",
        "        \n",
        "        model = DecisionTreeRegressor(max_depth=d, min_samples_leaf=samp)\n",
        "        model.fit(bow_train, y_train) # Train\n",
        "        \n",
        "        preds = model.predict(bow_train) # Get predictions\n",
        "        err = mean_squared_error(y_train['target'], preds) # Calculate error on trainset\n",
        "        tr_errors.append(err)\n",
        "        print(\"Mean Squared Error on train set: \", err)\n",
        "        \n",
        "        preds = model.predict(bow_cv) # Get predictions on CV set\n",
        "        err = mean_squared_error(y_cv['target'], preds) # Calculate error on cv set\n",
        "        cv_errors.append(err)\n",
        "        print(\"Mean Squared Error on cv set: \", err)\n",
        "        \n",
        "        if err < best_error: # Get best model trained\n",
        "            best_error = err\n",
        "            best_model = model\n",
        "        \n",
        "        print(\"*\"*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmn_h2wwFBRh"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "plt.suptitle(\"Hyperparameters vs MSE\")\n",
        "plt.plot(range(len(max_depth) * len(min_samples)), tr_errors)\n",
        "plt.plot(range(len(max_depth) * len(min_samples)), cv_errors)\n",
        "plt.legend(['train', 'cv'])\n",
        "plt.xticks(range(len(max_depth) * len(min_samples)), xticks, rotation=45)\n",
        "plt.xlabel('Hyperparameter: max depth + min_samples_leaf')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQWtwTSmFBRi"
      },
      "source": [
        "# Best models parameters:\n",
        "best_model.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBqK_xX5FBRi"
      },
      "source": [
        "##### 1.2.2 Feature Importance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqFAkhyuFBRi"
      },
      "source": [
        "weights = best_model.feature_importances_\n",
        "df = pd.DataFrame(data=weights, columns=['weights'], index=feat_names)\n",
        "df.sort_values(\"weights\", ascending=False).iloc[0:20,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lHwk0llFBRx"
      },
      "source": [
        "## Conclusion:\n",
        "\n",
        "\n",
        "\n",
        "1. **BagOfWords:**\n",
        "    - _SGDRegressor:_\n",
        "        - Hyperparameters Tuned Values: learning_rate(alpha): 1e-05 and penalty: l2\n",
        "        - Train MSE Loss: 0.02281\n",
        "        - CV MSE Loss: 0.02326\n",
        "    - _Decision Tree:_\n",
        "        - Hyperparameters Tuned Values: max_depth: 7 and min_samples_leaf: 100\n",
        "        - Train MSE Loss: 0.0310\n",
        "        - CV MSE Loss: 0.03128\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imV7mTFeFBRx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}